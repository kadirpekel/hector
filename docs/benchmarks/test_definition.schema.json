{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Hector Benchmark Test Definition",
  "description": "Schema for defining A/B tests and benchmark configurations",
  "type": "object",
  "required": ["test_name", "description", "variants", "scenarios"],
  "properties": {
    "test_name": {
      "type": "string",
      "description": "Unique identifier for this test"
    },
    "description": {
      "type": "string",
      "description": "Human-readable description of what this test evaluates"
    },
    "test_type": {
      "type": "string",
      "enum": ["ab_test", "benchmark", "regression"],
      "description": "Type of test",
      "default": "ab_test"
    },
    "variants": {
      "type": "array",
      "description": "Configuration variants to test",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["name", "config"],
        "properties": {
          "name": {
            "type": "string",
            "description": "Variant name (e.g., 'control', 'treatment_a')"
          },
          "description": {
            "type": "string",
            "description": "What makes this variant unique"
          },
          "config": {
            "oneOf": [
              {
                "type": "string",
                "description": "Path to YAML config file"
              },
              {
                "type": "object",
                "description": "Inline config modifications"
              }
            ]
          },
          "config_overrides": {
            "type": "object",
            "description": "Key-value pairs to override in the base config",
            "additionalProperties": true
          }
        }
      }
    },
    "scenarios": {
      "type": "array",
      "description": "Test scenarios to run against each variant",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["name", "prompt"],
        "properties": {
          "name": {
            "type": "string",
            "description": "Scenario name"
          },
          "prompt": {
            "type": "string",
            "description": "User prompt/query"
          },
          "expected_output_keywords": {
            "type": "array",
            "description": "Keywords expected in successful output",
            "items": {"type": "string"}
          },
          "success_criteria": {
            "type": "object",
            "description": "Custom success validation",
            "properties": {
              "max_tokens": {"type": "number"},
              "max_iterations": {"type": "number"},
              "required_tools": {
                "type": "array",
                "items": {"type": "string"}
              }
            }
          }
        }
      }
    },
    "metrics": {
      "type": "object",
      "description": "Metrics to collect and compare",
      "properties": {
        "primary": {
          "type": "string",
          "enum": ["quality", "cost", "speed", "accuracy"],
          "description": "Primary metric for comparison"
        },
        "secondary": {
          "type": "array",
          "description": "Secondary metrics to track",
          "items": {
            "type": "string",
            "enum": ["tokens", "iterations", "latency", "tool_usage"]
          }
        }
      }
    },
    "providers": {
      "type": "array",
      "description": "LLM providers to test (empty = all)",
      "items": {
        "type": "string",
        "enum": ["openai", "anthropic", "gemini"]
      }
    },
    "iterations": {
      "type": "number",
      "description": "Number of times to run each scenario",
      "default": 1,
      "minimum": 1
    }
  }
}

