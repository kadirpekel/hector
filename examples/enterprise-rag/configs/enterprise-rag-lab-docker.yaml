# Enterprise RAG Lab Configuration (Docker Compose)
# Multi-source RAG system for on-premise deployment
# This config uses Docker service names for inter-container communication
# Showcases advanced search features: hybrid search, re-ranking, multi-query

# Vector Database Options
# Qdrant is used by default, but you can switch to Weaviate, Milvus, or Chroma
vector_stores:
  qdrant:
    type: "qdrant"
    host: "qdrant"  # Docker service name
    port: 6334

# Embedder (Ollama)
embedders:
  embedder:
    type: "ollama"
    host: "http://ollama:11434"  # Docker service name
    model: "nomic-embed-text"
    timeout: 30

# SQL Database for Knowledge Base
databases:
  knowledge_db:
    driver: "postgres"
    host: "postgres"  # Docker service name
    port: 5432  # Internal port (not mapped port)
    database: "knowledge_base"
    username: "hector"
    password: "lab_password_123"
    ssl_mode: "disable"
    max_conns: 25
    max_idle: 5

# LLM Providers
# Using Ollama for 100% on-premise deployment
llms:
  local-llm:
    type: "ollama"
    host: "http://ollama:11434"  # Docker service name
    model: "qwen3"  # Native tool calling support
    temperature: 0.7
    timeout: 300  # Increased timeout for qwen3
  
  # For re-ranking and advanced search features
  # Note: In production, use a fast/cheap LLM for re-ranking
  # For this example, we'll use the same LLM (qwen3) for simplicity
  reranker:
    type: "ollama"
    host: "http://ollama:11434"
    model: "qwen3"
    temperature: 0.0  # Deterministic for ranking
    timeout: 60

# Document Stores - Multi-Source Configuration
document_stores:
  # Source 1: Internal Documentation (Directory)
  internal_docs:
    source: "directory"
    path: "/examples/enterprise-rag/docs/internal/"  # Mounted volume path
    include_patterns: ["*.md", "*.txt"]
    chunk_size: 1024
    chunk_overlap: 100

  # Source 2: Knowledge Base Articles (SQL Database - PostgreSQL)
  knowledge_base:
    source: "sql"
    sql:
      driver: "postgres"
      host: "postgres"  # Docker service name
      port: 5432  # Internal port
      database: "knowledge_base"
      username: "hector"
      password: "lab_password_123"
      ssl_mode: "disable"
    sql_tables:
      - table: "knowledge_articles"
        columns: ["title", "content"]
        id_column: "id"
        updated_column: "updated_at"
        where_clause: "status = 'published'"
        metadata_columns: ["category", "author", "created_at"]
    chunk_size: 800
    chunk_overlap: 50

  # Source 3: Internal Wiki (REST API)
  wiki_content:
    source: "api"
    api:
      base_url: "http://wiki-api:8080"  # Docker service name + internal port
      endpoints:
        - path: "/api/pages"
          method: "GET"
          id_field: "id"
          content_field: "title,content"
          metadata_fields: ["author", "category", "published_at", "updated_at"]
    chunk_size: 800
    chunk_overlap: 50

# Enterprise Knowledge Assistant Agent
agents:
  enterprise_assistant:
    llm: "local-llm"
    vector_store: "qdrant"
    embedder: "embedder"
    document_stores: ["internal_docs", "knowledge_base", "wiki_content"]
    
    tools:
      - "search"  # Enable semantic search
      - "evaluate_rag"  # Enable RAG evaluation tool
    
    prompt:
      include_context: false  # Use search tool explicitly
      system_prompt: |
        You are an enterprise knowledge assistant for an on-premise deployment.
        You have access to:
        - Internal documentation
        - Knowledge base articles from the database
        - Internal wiki content
        
        Always cite your sources when answering questions. If information is not
        available in the indexed documents, say so clearly.
        
        Focus on providing accurate, actionable information based on the
        enterprise knowledge base.
    
    search:
      top_k: 10  # Default number of results for RAG context
      threshold: 0.5  # Minimum similarity score (50%)
      preserve_case: true  # Preserve case for code/documentation
      
      # Advanced Search Configuration
      # Option 1: Hybrid Search (keyword + vector) - Recommended for enterprise
      search_mode: "hybrid"  # "vector", "hybrid", "keyword", "multi_query", or "hyde"
      hybrid_alpha: 0.6  # 60% vector, 40% keyword (balanced for enterprise docs)
      
      # Option 2: Multi-Query Expansion (improves recall for ambiguous queries)
      # search_mode: "multi_query"
      # multi_query:
      #   enabled: true
      #   llm: "reranker"
      #   num_variations: 3
      
      # Option 3: HyDE (Hypothetical Document Embeddings)
      # search_mode: "hyde"
      # hyde:
      #   enabled: true
      #   llm: "reranker"
      
      # LLM-based Re-ranking (improves result quality)
      # Note: Re-ranking requires an LLM. For production, use a fast/cheap model.
      rerank:
        enabled: true
        llm: "reranker"  # Use dedicated re-ranking LLM
        max_results: 20  # Re-rank top 20 results

# Observability
global:
  observability:
    enable_metrics: true
    tracing:
      enabled: true
      endpoint_url: "localhost:4317"  # OTLP gRPC endpoint (host:port format, not URL)

