# Enterprise RAG Lab Configuration
# Multi-source RAG system for on-premise deployment
# Showcases advanced search features: hybrid search, re-ranking, multi-query

# Vector Database Options
# Choose one based on your needs:
# - Qdrant: Self-hosted, production-ready (default)
# - Weaviate: Native hybrid search support
# - Milvus: High-performance, large-scale
# - Chroma: Lightweight, embedded
# - Pinecone: Managed cloud service

vector_stores:
  qdrant:
    type: "qdrant"
    host: "localhost"
    port: 6334
  
  # Alternative: Weaviate with native hybrid search
  # weaviate:
  #   type: "weaviate"
  #   host: "localhost"
  #   port: 8080

# Embedder (Ollama)
embedders:
  embedder:
    type: "ollama"
    host: "http://localhost:11434"
    model: "nomic-embed-text"
    timeout: 30

# SQL Database for Knowledge Base
databases:
  knowledge_db:
    driver: "postgres"
    host: "localhost"
    port: 5432
    database: "knowledge_base"
    username: "hector"
    password: "lab_password_123"
    ssl_mode: "disable"
    max_conns: 25
    max_idle: 5

# LLM Providers
# Using OpenAI for faster testing and re-ranking
# In production, use Ollama for 100% on-premise deployment
llms:
  local-llm:
    type: "openai"
    model: "gpt-4o-mini"  # Fast model for testing
    api_key: "${OPENAI_API_KEY}"
    temperature: 0.7
    timeout: 60
  
  # For re-ranking and advanced search features
  reranker:
    type: "openai"
    model: "gpt-4o-mini"  # Fast, cost-effective for re-ranking
    api_key: "${OPENAI_API_KEY}"
    temperature: 0.0  # Deterministic for ranking
    timeout: 30

# Document Stores - Multi-Source Configuration
document_stores:
  # Source 1: Internal Documentation (Directory)
  internal_docs:
    source: "directory"
    path: "examples/enterprise-rag/docs/internal/"
    include_patterns: ["*.md", "*.txt"]
    chunk_size: 1024
    chunk_overlap: 100

  # Source 2: Knowledge Base Articles (SQL Database - PostgreSQL)
  knowledge_base:
    source: "sql"
    sql:
      driver: "postgres"
      host: "localhost"
      port: 5433  # Docker container port (mapped from 5432)
      database: "knowledge_base"
      username: "hector"
      password: "lab_password_123"
      ssl_mode: "disable"
    sql_tables:
      - table: "knowledge_articles"
        columns: ["title", "content"]
        id_column: "id"
        updated_column: "updated_at"
        where_clause: "status = 'published'"
        metadata_columns: ["category", "author", "created_at"]
    chunk_size: 800
    chunk_overlap: 50

  # Source 3: Internal Wiki (REST API)
  wiki_content:
    source: "api"
    api:
      base_url: "http://localhost:8081"
      endpoints:
        - path: "/api/pages"
          method: "GET"
          id_field: "id"
          content_field: "title,content"
          metadata_fields: ["author", "category", "published_at", "updated_at"]
    chunk_size: 800
    chunk_overlap: 50

# Enterprise Knowledge Assistant Agent
agents:
  enterprise_assistant:
    llm: "local-llm"
    vector_store: "qdrant"
    embedder: "embedder"
    document_stores: ["internal_docs", "knowledge_base", "wiki_content"]
    
    tools:
      - "search"  # Enable semantic search
      - "evaluate_rag"  # Enable RAG evaluation tool
    
    prompt:
      include_context: false  # Disable automatic RAG to test search tool usage
      system_prompt: |
        You are an enterprise knowledge assistant for an on-premise deployment.
        You have access to:
        - Internal documentation
        - Knowledge base articles from the database
        - Internal wiki content
        
        Always cite your sources when answering questions. If information is not
        available in the indexed documents, say so clearly.
        
        Focus on providing accurate, actionable information based on the
        enterprise knowledge base.
    
    search:
      top_k: 10  # Default number of results for RAG context
      threshold: 0.5  # Minimum similarity score (50%)
      preserve_case: true  # Preserve case for code/documentation
      
      # Advanced Search Configuration
      # Option 1: Hybrid Search (keyword + vector) - Recommended for enterprise
      search_mode: "hybrid"  # "vector", "hybrid", "keyword", "multi_query", or "hyde"
      hybrid_alpha: 0.6  # 60% vector, 40% keyword (balanced for enterprise docs)
      
      # Option 2: Multi-Query Expansion (improves recall for ambiguous queries)
      # search_mode: "multi_query"
      # multi_query:
      #   enabled: true
      #   llm: "reranker"
      #   num_variations: 3
      
      # Option 3: HyDE (Hypothetical Document Embeddings)
      # search_mode: "hyde"
      # hyde:
      #   enabled: true
      #   llm: "reranker"
      
      # LLM-based Re-ranking (improves result quality)
      rerank:
        enabled: true
        llm: "reranker"  # Use fast model for re-ranking
        max_results: 20  # Re-rank top 20 results

# Observability
global:
  observability:
    enable_metrics: true
    tracing:
      enabled: true
      endpoint_url: "localhost:4317"  # OTLP gRPC endpoint (host:port format, not URL)

