package tools

import (
	"context"
	"encoding/json"
	"fmt"
	"time"

	"github.com/kadirpekel/hector/pkg/databases"
	"github.com/kadirpekel/hector/pkg/evaluation"
	"github.com/kadirpekel/hector/pkg/llms"
)

// EvaluateRAGTool evaluates RAG system performance
type EvaluateRAGTool struct {
	evaluator evaluation.Evaluator
}

// NewEvaluateRAGTool creates a new RAG evaluation tool
func NewEvaluateRAGTool(llmProvider llms.LLMProvider) *EvaluateRAGTool {
	evaluator := evaluation.NewLLMEvaluator(llmProvider)
	return &EvaluateRAGTool{
		evaluator: evaluator,
	}
}

func (t *EvaluateRAGTool) GetName() string {
	return "evaluate_rag"
}

func (t *EvaluateRAGTool) GetDescription() string {
	return `Evaluates RAG (Retrieval-Augmented Generation) system performance by analyzing query, retrieved documents, and generated answer.

This tool calculates metrics including:
- Context Precision: Proportion of retrieved contexts that are relevant
- Context Recall: Proportion of relevant contexts that were retrieved  
- Answer Relevance: How relevant is the answer to the query
- Faithfulness: How faithful is the answer to the retrieved contexts
- Answer Correctness: Overall correctness score

Use this tool to measure and improve RAG system quality.`
}

func (t *EvaluateRAGTool) GetSchema() map[string]interface{} {
	return map[string]interface{}{
		"type": "object",
		"properties": map[string]interface{}{
			"query": map[string]interface{}{
				"type":        "string",
				"description": "The user query that was answered",
			},
			"retrieved_docs": map[string]interface{}{
				"type":        "array",
				"description": "Array of retrieved documents from search",
				"items": map[string]interface{}{
					"type": "object",
					"properties": map[string]interface{}{
						"id": map[string]interface{}{
							"type":        "string",
							"description": "Document ID",
						},
						"content": map[string]interface{}{
							"type":        "string",
							"description": "Document content",
						},
						"score": map[string]interface{}{
							"type":        "number",
							"description": "Relevance score",
						},
					},
				},
			},
			"generated_answer": map[string]interface{}{
				"type":        "string",
				"description": "The answer generated by the RAG system",
			},
			"ground_truth": map[string]interface{}{
				"type":        "string",
				"description": "Optional ground truth answer for comparison",
			},
		},
		"required": []string{"query", "retrieved_docs", "generated_answer"},
	}
}

func (t *EvaluateRAGTool) GetInfo() ToolInfo {
	return ToolInfo{
		Name:        t.GetName(),
		Description: t.GetDescription(),
		Parameters:  []ToolParameter{},
	}
}

func (t *EvaluateRAGTool) Execute(ctx context.Context, args map[string]interface{}) (ToolResult, error) {
	// Extract arguments
	query, ok := args["query"].(string)
	if !ok || query == "" {
		return ToolResult{
			Success:  false,
			Content:  "query is required and must be a string",
			Error:    "invalid query argument",
			ToolName: t.GetName(),
		}, nil
	}

	generatedAnswer, ok := args["generated_answer"].(string)
	if !ok || generatedAnswer == "" {
		return ToolResult{
			Success:  false,
			Content:  "generated_answer is required and must be a string",
			Error:    "invalid generated_answer argument",
			ToolName: t.GetName(),
		}, nil
	}

	groundTruth, _ := args["ground_truth"].(string)

	// Parse retrieved documents
	retrievedDocsRaw, ok := args["retrieved_docs"].([]interface{})
	if !ok {
		return ToolResult{
			Success:  false,
			Content:  "retrieved_docs must be an array",
			Error:    "invalid retrieved_docs argument",
			ToolName: t.GetName(),
		}, nil
	}

	retrievedDocs := make([]databases.SearchResult, 0, len(retrievedDocsRaw))
	for _, docRaw := range retrievedDocsRaw {
		docMap, ok := docRaw.(map[string]interface{})
		if !ok {
			continue
		}

		doc := databases.SearchResult{}
		if id, ok := docMap["id"].(string); ok {
			doc.ID = id
		}
		if content, ok := docMap["content"].(string); ok {
			doc.Content = content
		}
		if score, ok := docMap["score"].(float64); ok {
			doc.Score = float32(score)
		}

		// Extract metadata
		doc.Metadata = make(map[string]interface{})
		for k, v := range docMap {
			if k != "id" && k != "content" && k != "score" {
				doc.Metadata[k] = v
			}
		}

		retrievedDocs = append(retrievedDocs, doc)
	}

	// Run evaluation
	startTime := time.Now()
	result, err := t.evaluator.Evaluate(ctx, query, retrievedDocs, generatedAnswer, groundTruth)
	if err != nil {
		return ToolResult{
			Success:  false,
			Content:  fmt.Sprintf("Evaluation failed: %v", err),
			Error:    err.Error(),
			ToolName: t.GetName(),
		}, nil
	}

	latency := time.Since(startTime).Seconds()
	result.Metrics.Latency = latency

	// Format result as JSON
	resultJSON, err := json.MarshalIndent(result, "", "  ")
	if err != nil {
		return ToolResult{
			Success:  false,
			Content:  fmt.Sprintf("Failed to serialize result: %v", err),
			Error:    err.Error(),
			ToolName: t.GetName(),
		}, nil
	}

	// Create summary
	summary := fmt.Sprintf(`RAG Evaluation Results:

Query: %s
Context Precision: %.2f
Context Recall: %.2f
Answer Relevance: %.2f
Faithfulness: %.2f
Answer Correctness: %.2f
Latency: %.2fs

Full results:
%s`, query, result.Metrics.ContextPrecision, result.Metrics.ContextRecall,
		result.Metrics.AnswerRelevance, result.Metrics.Faithfulness,
		result.Metrics.AnswerCorrectness, result.Metrics.Latency, string(resultJSON))

	return ToolResult{
		Success:       true,
		Content:       summary,
		ToolName:      t.GetName(),
		ExecutionTime: time.Since(startTime),
		Metadata: map[string]interface{}{
			"metrics": result.Metrics,
			"result":  result,
		},
	}, nil
}
