# Basic Rate Limiting Configuration Example
# This example shows how to configure rate limiting for session stores

llms:
  gpt-4-llm:
    type: "openai"
    model: "gpt-4"
    api_key: "${OPENAI_API_KEY}"
    temperature: 0.7
    max_tokens: 2000

# Session stores configuration
session_stores:
  # Example 1: In-memory session store with basic rate limiting
  default:
    backend: memory
    rate_limit:
      enabled: true
      scope: session  # Rate limit per session (each session has independent quota)
      backend: memory # Store rate limit data in memory
      limits:
        # Daily token limit (e.g., for cost control with LLM APIs)
        - type: token
          window: day
          limit: 50000
        
        # Per-minute request limit (e.g., to prevent burst traffic)
        - type: count
          window: minute
          limit: 30

  # Example 2: SQL-backed session store with user-level rate limiting
  persistent:
    backend: sql
    sql:
      driver: sqlite
      database: ./sessions.db
      max_conns: 25
      max_idle: 5
    rate_limit:
      enabled: true
      scope: user  # Rate limit per user (all sessions for a user share quota)
      backend: sql # Store rate limit data in SQL database
      limits:
        # Multiple time windows for comprehensive protection
        - type: count
          window: minute
          limit: 60
        - type: token
          window: hour
          limit: 10000
        - type: token
          window: day
          limit: 100000

# Agent configuration
agents:
  assistant:
    name: "Assistant"
    description: "A helpful AI assistant with rate limiting"
    llm: "gpt-4-llm"
    session_store: default  # Use the 'default' session store with rate limiting
    reasoning:
      engine: "default"
      max_iterations: 10
    prompt:
      system_prompt: |
        You are a helpful AI assistant. You are polite, concise, and efficient.


