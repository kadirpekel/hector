<div align="center">
  <img src="hector_logo.png" alt="Hector Logo">
</div>

[![Go Version](https://img.shields.io/badge/Go-1.24+-blue.svg)](https://golang.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Alpha-orange.svg)](https://github.com/kadirpekel/hector)

# Hector

> **Declarative AI Agent Framework** - Define once, deploy anywhere. Build sophisticated single agents and multi-agent systems through pure YAML configuration.

## Why Declarative?

Hector eliminates code for agent orchestration. Define your AI system's architecture, reasoning strategies, and workflows in YAML‚ÄîHector handles the complexity.

**Traditional Approach:**
```go
// 100+ lines of code
agent := NewAgent(llm)
agent.AddTool(commandTool)
agent.SetReasoning(chainOfThought)
agent.Run()
```

**Hector's Declarative Approach:**
```yaml
# config.yaml - that's it!
agents:
  assistant:
    llm: "openai"
    reasoning:
      engine: "chain-of-thought"
    tools: ["execute_command"]
```

## Table of Contents

- [Why Declarative?](#why-declarative)
- [Core Philosophy](#core-philosophy)
- [Features](#features)
- [Quick Start](#quick-start)
- [Multi-Agent Teams](#multi-agent-teams) ‚≠ê NEW
- [Architecture](#architecture)
- [Configuration](#configuration)
- [Examples](#examples)
- [CLI Reference](#cli-reference)
- [Supported Providers](#supported-providers)
- [Performance](#performance) ‚≠ê NEW
- [License](#license)

## Core Philosophy

### 1. **Configuration Over Code**
Define your AI system's behavior, not its implementation. Focus on what you want, not how to build it.

### 2. **Composable by Design**
Mix and match LLM providers, reasoning engines, tools, and agents like building blocks. Everything is pluggable.

### 3. **Streaming-First**
Real-time output with intelligent buffering, marker detection, and optimized response handling.

### 4. **Complexity Hidden, Power Exposed**
Simple things are simple, complex things are possible. Start with 5 lines of YAML, scale to enterprise workflows.

---

## Features

### Single Agent Capabilities

- **Two Reasoning Engines**: Choose your strategy declaratively
  - `chain-of-thought`: Fast, behavioral-signal-based (simple queries)
  - `structured-reasoning`: Goal-oriented with meta-cognition (complex analysis)
  - **Thinking Mode**: See internal reasoning in grayed-out blocks (like Claude in Cursor)

- **Universal Tool Integration**
  - **Local Tools**: Secure command execution with sandboxing
  - **MCP Protocol**: Connect to external tool servers
  - **Custom Tools**: Extend with your own implementations
  
- **Smart Context Management**
  - Vector search with Qdrant
  - Conversation history with intelligent truncation
  - Document stores with automatic indexing
  - Semantic search across knowledge bases

- **Production-Grade Streaming**
  - Real-time output with intelligent buffering
  - Marker detection for structured responses
  - Zero-allocation optimizations
  - User-friendly masking (no raw JSON)

### Multi-Agent Team Capabilities ‚≠ê NEW

- **Real-Time Streaming**: Watch agents work in real-time with progress tracking
- **Two Execution Modes**:
  - **DAG Mode**: Define dependencies, sequential/parallel execution
  - **Autonomous Mode**: AI-driven task planning and dynamic collaboration
- **Specialized Agents**: Each agent has its own LLM, reasoning strategy, and tools
- **Live Progress Tracking**: See which agent is running, completion percentage, real-time output
- **Production-Grade Performance**: Linear O(n) scaling, minimal memory footprint (<40KB for 20 agents)
- **Event-Based Architecture**: Stream workflow events (start, progress, output, complete)

### Universal Features

- **Single YAML Configuration**: Everything from providers to workflows
- **Zero-Config Mode**: Intelligent defaults (Ollama + local tools)
- **Provider Agnostic**: OpenAI, Anthropic (Claude), Ollama - switch declaratively
- **Extension System**: Pluggable tools, memory, custom capabilities
- **Clean Architecture**: SOLID principles, dependency injection, no tight coupling

---

## See It In Action

Here's Hector's **structured reasoning** engine with **thinking mode** enabled:

**Query:** "What is 2+2 and why is it important in mathematics?"

**Output:**
```bash
# Internal reasoning
[Thinking: Goal identified: Answer the question about 2+2 and explain its significance]
[Thinking:   Sub-goal 1: Calculate the sum of 2+2]
[Thinking:   Sub-goal 2: Research and describe the role of basic arithmetic]
[Thinking:   Sub-goal 3: Explain foundational importance]

[Thinking: Iteration 1/10: reasoning]

# Agent response
To address your query, I will first calculate 2+2 and then explain its significance.

The sum of 2+2 is 4.

Role of Basic Arithmetic in Mathematics:
Basic arithmetic operations form the foundation of mathematics. Understanding these
operations is crucial because:

- Foundation for Advanced Topics: Mastery of basic arithmetic is essential for
  learning algebra, geometry, and calculus.
- Everyday Applications: Used in budgeting, shopping, cooking - making it practical.
- Problem Solving: Develops logical thinking valuable in academic and real-world scenarios.

# Quality check
[Thinking: Quality check: 100% confident]
[Thinking:   Decision: Ready to answer - comprehensively addressed main goal and sub-goals]
```

Notice:
- ‚úÖ `[Thinking: ...]` blocks show internal reasoning (appear dimmed in terminal)
- ‚úÖ Goal extraction and sub-goal tracking
- ‚úÖ Confidence-based stopping (100% = done)
- ‚úÖ Meta-cognitive reflection throughout

**Try it yourself:**
```bash
./hector --config local-configs/structured-reasoning.yaml
```

---

## Quick Start

### Prerequisites

```bash
# For local LLMs (optional)
ollama serve

# For vector search (optional)
docker run -p 6333:6333 qdrant/qdrant
```

### Installation

```bash
git clone https://github.com/kadirpekel/hector.git
cd hector
go build ./cmd/hector
```

### Your First Agent

Create `config.yaml`:

```yaml
agents:
  assistant:
    llm: "openai"

llms:
  openai:
    type: "openai"
    model: "gpt-4o-mini"
    api_key: "${OPENAI_API_KEY}"
```

Run:

```bash
export OPENAI_API_KEY="your-key-here"
./hector --config config.yaml
```

**That's it!** Your declarative AI agent is running.

### Add Tools (Declaratively)

```yaml
agents:
  assistant:
    llm: "openai"

llms:
  openai:
    type: "openai"
    model: "gpt-4o-mini"
    api_key: "${OPENAI_API_KEY}"

# Just declare tools - Hector handles the rest
tools:
  repositories:
    - name: "local"
      type: "local"
      tools:
        - name: "execute_command"
          type: "command"
          config:
            allowed_commands: ["ls", "cat", "grep", "git"]
```

### Switch Reasoning Engines (Declaratively)

```yaml
agents:
  # Fast reasoning
  quick-agent:
    llm: "openai"
    reasoning:
      engine: "chain-of-thought"
      max_iterations: 5

  # Thorough reasoning with visible thinking
  deep-agent:
    llm: "openai"
    reasoning:
      engine: "structured-reasoning"
      max_iterations: 10
      show_thinking: true
```

---

## Multi-Agent Teams

### Real-Time Streaming Multi-Agent Workflows ‚≠ê NEW

Hector now features **world-class multi-agent orchestration** with real-time streaming, live progress tracking, and production-grade performance.

#### See It In Action

**DAG Workflow** (Sequential execution with dependencies):

```bash
$ echo "Research and analyze renewable energy trends" | ./hector --workflow examples/workflow.yaml

üöÄ Starting workflow: Research Analysis Workflow
------------------------------------------------------------

ü§ñ Starting agent: research-agent
[Streaming output in real-time...]
Researching renewable energy market trends...
Found 15 relevant sources...
Analyzing growth patterns...
‚úÖ Agent research-agent completed in 12.3s

üìä Progress: 33.3% (1/3 steps)

ü§ñ Starting agent: analysis-agent
[Streaming output in real-time...]
Analyzing research findings...
Identifying key patterns...
Solar: 23% growth, Wind: 18% growth...
‚úÖ Agent analysis-agent completed in 15.7s

üìä Progress: 66.7% (2/3 steps)

ü§ñ Starting agent: synthesis-agent
[Streaming output in real-time...]
Synthesizing insights...
Creating comprehensive report...
‚úÖ Agent synthesis-agent completed in 10.2s

------------------------------------------------------------
‚úÖ Workflow completed in 38.2s!
```

Notice:
- ‚úÖ **Real-time output** - See agent responses as they're generated
- ‚úÖ **Progress tracking** - Know exactly where you are in the workflow
- ‚úÖ **Sequential execution** - Each agent completes before the next starts
- ‚úÖ **Live timing** - Duration for each agent and total workflow

#### Two Execution Modes

**1. DAG Mode (Directed Acyclic Graph)**

Define dependencies and execution order:

```yaml
workflows:
  research-workflow:
    name: "Research Analysis"
    mode: "dag"  # Structured execution
    
    agents:
      - "researcher"    # Gathers information
      - "analyzer"      # Analyzes findings
      - "synthesizer"   # Creates report
    
    # Optional: Define explicit dependencies
    execution:
      dag:
        steps:
          - name: "research"
            agent: "researcher"
          - name: "analyze"
            agent: "analyzer"
            depends_on: ["research"]
          - name: "synthesize"
            agent: "synthesizer"
            depends_on: ["analyze"]
```

**Use Cases**:
- Research ‚Üí Analysis ‚Üí Report generation
- Data collection ‚Üí Processing ‚Üí Visualization
- Code generation ‚Üí Testing ‚Üí Documentation

**2. Autonomous Mode (AI-Driven)**

Let AI dynamically coordinate agents:

```yaml
workflows:
  autonomous-workflow:
    name: "Autonomous Problem Solver"
    mode: "autonomous"  # Dynamic coordination
    
    agents:
      - "planner"      # Creates execution plan
      - "executor-1"   # Executes tasks
      - "executor-2"   # Parallel execution
      - "validator"    # Validates results
    
    settings:
      max_iterations: 10
      quality_threshold: 0.8
```

**Use Cases**:
- Complex problem-solving with unknown steps
- Dynamic task decomposition
- Self-organizing agent teams

#### Streaming Architecture

Every workflow event is streamed in real-time:

```go
Event Types:
  ‚Ä¢ workflow_start   - Workflow begins
  ‚Ä¢ agent_start      - Agent starts execution
  ‚Ä¢ agent_output     - Real-time agent output (streamed)
  ‚Ä¢ agent_complete   - Agent finishes
  ‚Ä¢ progress         - Progress update (X/Y steps, percentage)
  ‚Ä¢ workflow_end     - Workflow completes
```

**Benefits**:
- **User Experience**: No waiting in the dark - see progress instantly
- **Debugging**: Watch exactly what each agent does
- **Monitoring**: Track execution in production
- **Cancellation**: Stop workflows mid-execution

#### Performance Characteristics

Tested and benchmarked on Apple M2:

| Agents | Execution Time | Memory | Status |
|--------|----------------|--------|--------|
| 1 | 303ms | 13.4 KB | ‚úÖ |
| 2 | 606ms | 14.0 KB | ‚úÖ |
| 5 | 1.51s | 15.8 KB | ‚úÖ |
| 10 | 3.03s | 21.3 KB | ‚úÖ |
| 20 | 6.06s | 35.3 KB | ‚úÖ |

**Key Findings**:
- **Perfect Linear Scaling**: O(n) - Predictable performance
- **Low Memory**: ~1.1 KB per agent overhead
- **No Bottlenecks**: Workflow engine adds <1% overhead vs real LLM execution
- **DAG = Autonomous**: Both modes have identical performance

See [BENCHMARK_RESULTS.md](BENCHMARK_RESULTS.md) for comprehensive performance analysis.

#### Quick Start - Multi-Agent Workflow

**1. Create workflow config** (`my-workflow.yaml`):

```yaml
version: "1.0"
name: "my-workflow"

# Define your LLMs
llms:
  main:
    type: "openai"
    model: "gpt-4o-mini"
    api_key: "${OPENAI_API_KEY}"

# Define specialized agents
agents:
  researcher:
    name: "Research Agent"
    llm: "main"
    prompt:
      system_prompt: "You are a research specialist. Gather comprehensive information."
  
  analyzer:
    name: "Analysis Agent"
    llm: "main"
    prompt:
      system_prompt: "You are an analysis specialist. Identify patterns and insights."

# Define workflow
workflows:
  my-workflow:
    name: "Research and Analysis"
    mode: "dag"
    agents:
      - "researcher"
      - "analyzer"
```

**2. Run the workflow**:

```bash
export OPENAI_API_KEY="your-key"
echo "Analyze AI market trends" | ./hector --workflow my-workflow.yaml
```

**3. Watch it stream in real-time!**

---

## Architecture

### Declarative Configuration Flow

```
YAML Config ‚Üí Validation ‚Üí Component Registry ‚Üí Agent Runtime
     ‚Üì              ‚Üì               ‚Üì                  ‚Üì
  Define        Check Rules    Providers Load     Execute
  What You      Defaults       Services Ready     Streaming
  Want          Applied        Extensions Reg     Real-time
```

### System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   DECLARATIVE LAYER (YAML)                      ‚îÇ
‚îÇ  User defines: agents, providers, tools, workflows, reasoning   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   CONFIGURATION LAYER                           ‚îÇ
‚îÇ  ‚Ä¢ Validation    ‚Ä¢ Defaults    ‚Ä¢ Provider Registry             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   COMPONENT MANAGER                             ‚îÇ
‚îÇ  ‚Ä¢ LLM Factory   ‚Ä¢ Tool Registry   ‚Ä¢ Extension Service          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   AGENT RUNTIME                                 ‚îÇ
‚îÇ  ‚Ä¢ Reasoning Engine   ‚Ä¢ Service Injection   ‚Ä¢ Streaming         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   EXECUTION                                     ‚îÇ
‚îÇ  ‚Ä¢ LLM Calls   ‚Ä¢ Tool Execution   ‚Ä¢ Context Management          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Multi-Agent Workflow Architecture ‚≠ê NEW

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      TEAM ORCHESTRATION                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  Team.ExecuteStreaming()                                        ‚îÇ
‚îÇ       ‚îÇ                                                         ‚îÇ
‚îÇ       ‚ñº                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  WorkflowExecutor (DAG / Autonomous)                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Streams events in real-time                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Tracks progress (steps, percentage)                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Coordinates agent execution                           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ           ‚îÇ                                   ‚îÇ                 ‚îÇ
‚îÇ           ‚ñº                                   ‚ñº                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ  Agent 1        ‚îÇ              ‚îÇ  Agent 2        ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Own LLM      ‚îÇ              ‚îÇ  ‚Ä¢ Own LLM      ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Own Tools    ‚îÇ              ‚îÇ  ‚Ä¢ Own Tools    ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Own Reasoning‚îÇ              ‚îÇ  ‚Ä¢ Own Reasoning‚îÇ          ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Streams ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚Ä¢ Streams        ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ           ‚îÇ                                   ‚îÇ                 ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ                        ‚ñº                                        ‚îÇ
‚îÇ               Event Stream (chan)                               ‚îÇ
‚îÇ               ‚Ä¢ workflow_start                                  ‚îÇ
‚îÇ               ‚Ä¢ agent_start                                     ‚îÇ
‚îÇ               ‚Ä¢ agent_output  ‚óÑ‚îÄ‚îÄ Real-time!                   ‚îÇ
‚îÇ               ‚Ä¢ agent_complete                                  ‚îÇ
‚îÇ               ‚Ä¢ progress                                        ‚îÇ
‚îÇ               ‚Ä¢ workflow_end                                    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Features**:
- ‚úÖ **AgentFactory** - Single source of truth for agent creation (no duplication)
- ‚úÖ **Event-Based** - All workflow events streamed in real-time
- ‚úÖ **Progress Tracking** - Accurate step counting and percentage calculation
- ‚úÖ **Zero Coupling** - Each agent independent, fully isolated
- ‚úÖ **Clean Architecture** - SOLID principles, dependency injection

### Agent Services Architecture

Every agent gets injected with services (dependency injection):

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         AGENT RUNTIME                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   Reasoning  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  Extension   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ     LLM      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    Engine    ‚îÇ     ‚îÇ   Service    ‚îÇ     ‚îÇ   Service    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ     ‚îÇ              ‚îÇ     ‚îÇ              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Execute    ‚îÇ     ‚îÇ ‚Ä¢ Register   ‚îÇ     ‚îÇ ‚Ä¢ Generate   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Orchestrate‚îÇ     ‚îÇ ‚Ä¢ Process    ‚îÇ     ‚îÇ ‚Ä¢ Stream     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Iterate    ‚îÇ     ‚îÇ ‚Ä¢ Execute    ‚îÇ     ‚îÇ ‚Ä¢ Mask       ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ         ‚îÇ                     ‚îÇ                     ‚îÇ          ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                              ‚îÇ                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                      ‚îÇ                      ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  Tools  ‚îÇ          ‚îÇ  Context  ‚îÇ         ‚îÇ  History  ‚îÇ
   ‚îÇExtension‚îÇ          ‚îÇ  Service  ‚îÇ         ‚îÇ  Service  ‚îÇ
   ‚îÇ         ‚îÇ          ‚îÇ           ‚îÇ         ‚îÇ           ‚îÇ
   ‚îÇ‚Ä¢ Execute‚îÇ          ‚îÇ‚Ä¢ Search   ‚îÇ         ‚îÇ‚Ä¢ Store    ‚îÇ
   ‚îÇ‚Ä¢ Command‚îÇ          ‚îÇ‚Ä¢ Vector   ‚îÇ         ‚îÇ‚Ä¢ Retrieve ‚îÇ
   ‚îÇ‚Ä¢ MCP    ‚îÇ          ‚îÇ‚Ä¢ Index    ‚îÇ         ‚îÇ‚Ä¢ Truncate ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Extension System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      LLM STREAMING FLOW                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                ‚îÇ
‚îÇ  Raw Chunks ‚Üí Buffer ‚Üí Marker Detection ‚Üí Masking ‚Üí Output    ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  "Let me help"  ‚îê                                             ‚îÇ
‚îÇ  " you. TOOL_"  ‚îú‚îÄ‚ñ∫ Accumulate ‚îÄ‚ñ∫ ContainsMarker() ‚îÄ‚ñ∫ Found! ‚îÇ
‚îÇ  "CALLS: {...}" ‚îò        ‚îÇ              ‚îÇ              ‚îÇ      ‚îÇ
‚îÇ                          ‚îÇ              ‚îÇ              ‚îÇ      ‚îÇ
‚îÇ                    InputBuffer    ExtensionService   Stream   ‚îÇ
‚îÇ                     (Efficient)     (Delegates)    (Masked)   ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  Features:                                                     ‚îÇ
‚îÇ  ‚úì Split-marker detection  ‚úì Dynamic buffering               ‚îÇ
‚îÇ  ‚úì Zero allocations        ‚úì Real-time masking               ‚îÇ
‚îÇ  ‚úì Generic (any extension) ‚úì Production-optimized            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Configuration

> üìö **For complete configuration documentation, see [CONFIGURATION.md](CONFIGURATION.md)**

### Quick Start Configuration

**Minimal Configuration:**
```yaml
agents:
  assistant:
    llm: "openai"

llms:
  openai:
    type: "openai"
    model: "gpt-4o-mini"
    api_key: "${OPENAI_API_KEY}"
```

**With Tools:**
```yaml
agents:
  assistant:
    llm: "openai"

llms:
  openai:
    type: "openai"
    model: "gpt-4o"
    api_key: "${OPENAI_API_KEY}"

tools:
  repositories:
    - name: "local"
      type: "local"
      tools:
        - name: "execute_command"
          type: "command"
          config:
            allowed_commands: ["ls", "cat", "grep", "git"]
```

**Reasoning Engines:**
```yaml
agents:
  # Fast reasoning (chain-of-thought)
  fast-agent:
    llm: "openai"
    reasoning:
      engine: "chain-of-thought"
      max_iterations: 5

  # Thorough reasoning (structured-reasoning)
  thorough-agent:
    llm: "openai"
    reasoning:
      engine: "structured-reasoning"
      max_iterations: 10
      show_thinking: true    # See internal reasoning!
```

### Configuration Sections

All declaratively defined in YAML:

- **Global Settings** - Logging, performance, timeouts
- **LLM Providers** - OpenAI, Anthropic (Claude), Ollama
- **Databases** - Qdrant vector database
- **Embedders** - Ollama embeddings
- **Agents** - AI agent configuration
- **Reasoning Engines** - chain-of-thought vs structured-reasoning
- **Tools** - Command execution, MCP integration
- **Document Stores** - Knowledge base configuration
- **Workflows** - Multi-agent orchestration

See [CONFIGURATION.md](CONFIGURATION.md) for complete reference with all options, defaults, and best practices.

---

## Examples

> üìñ **For detailed examples documentation, see [examples/README.md](examples/README.md)**

### Available Examples

| Example | Description | Use Case |
|---------|-------------|----------|
| [zero-config.yaml](examples/zero-config.yaml) | Zero configuration | Quick start |
| [minimal.yaml](examples/minimal.yaml) | Minimal setup | Production single agent |
| [basic.yaml](examples/basic.yaml) | With tools & search | General purpose |
| [chain-of-thought.yaml](examples/chain-of-thought.yaml) | Fast reasoning | Speed matters |
| [reasoning-comparison.yaml](examples/reasoning-comparison.yaml) | Compare engines | Learn differences |
| [workflow.yaml](examples/workflow.yaml) | Multi-agent DAG | Structured workflows |
| [advanced.yaml](examples/advanced.yaml) | Enterprise | Complex systems |

### Quick Example

```bash
# Try the basic example
./hector --config examples/basic.yaml

# Try chain-of-thought reasoning
./hector --config examples/chain-of-thought.yaml

# Try multi-agent workflow
echo "Research renewable energy" | ./hector --workflow examples/workflow.yaml
```

See [examples/README.md](examples/README.md) for detailed descriptions, configuration breakdowns, and usage instructions for each example.

---

## CLI Reference

### Single Agent Mode

```bash
./hector [options]

Options:
  --config string     YAML configuration file path
  --agent string      Agent name to use (defaults to first agent)
  --debug            Show technical details and debug info

Examples:
# Interactive mode
./hector --config config.yaml

# Single query mode (pipe input)
echo "What is AI?" | ./hector --config config.yaml

# Debug mode
./hector --config config.yaml --debug

# Specific agent
./hector --config config.yaml --agent researcher
```

### Multi-Agent Workflow Mode

```bash
./hector --workflow <config.yaml>

Options:
  --workflow string   Workflow configuration file path
  --debug            Show technical details and debug info

Examples:
# Run DAG workflow
echo "Research topic" | ./hector --workflow examples/workflow.yaml

# Run autonomous workflow
echo "Analyze data" | ./hector --workflow examples/advanced.yaml
```

### Interactive Commands

```bash
/help         # Show available commands
/tools        # List available tools
/quit         # Exit the application
```

---

## Supported Providers

All providers are **declaratively configured** - just change the YAML:

### LLM Providers

- **OpenAI**: gpt-4o, gpt-4o-mini, gpt-3.5-turbo
- **Anthropic**: Claude Sonnet 4.5, Claude Opus 4.1, Claude Haiku
- **Ollama**: All supported models (Llama, Mistral, CodeLlama, etc.)

**Switch providers declaratively:**
```yaml
# Use OpenAI
llms:
  main: { type: "openai", model: "gpt-4o" }

# Switch to Claude
llms:
  main: { type: "anthropic", model: "claude-sonnet-4.5-20250514" }

# Use local Ollama
llms:
  main: { type: "ollama", model: "llama3.2" }
```

### Database Providers

- **Qdrant**: Vector similarity search with collections and filtering

### Embedding Providers

- **Ollama**: nomic-embed-text, all-MiniLM-L6-v2, and other models

### Tool Repositories

- **Local**: Built-in command execution with sandboxing
- **MCP**: Model Context Protocol for external tools

---

## Performance

### Multi-Agent Workflow Benchmarks

Comprehensive testing on Apple M2 (ARM64):

**Scaling Performance**:
```
Time = n √ó 300ms (perfect linear O(n))
Memory = 13KB + (n √ó 1.1KB)

1 agent:   303ms,  13.4 KB  ‚úÖ
10 agents: 3.03s,  21.3 KB  ‚úÖ
20 agents: 6.06s,  35.3 KB  ‚úÖ
```

**Key Metrics**:
- ‚úÖ **Perfect Linear Scaling** - No exponential slowdown
- ‚úÖ **Low Memory** - Only 1.1 KB per agent overhead
- ‚úÖ **Event Throughput** - 23 events/sec
- ‚úÖ **Negligible Overhead** - <1% vs real LLM execution time

**Production Reality**:
- Simple agent (5s): Workflow overhead = 6%
- Complex agent (30s): Workflow overhead = 1%
- Deep analysis (120s): Workflow overhead = 0.25%

**Workflow overhead is negligible in production!**

See [BENCHMARK_RESULTS.md](BENCHMARK_RESULTS.md) for detailed performance analysis with comprehensive test results.

---

## Why Hector?

### Declarative = Maintainable

**Change LLM provider?** Edit 1 line in YAML.
**Add reasoning?** Declare it, don't code it.
**Multi-agent workflow?** Define steps, Hector orchestrates.

### Built Right

- ‚úÖ Real-time streaming for single and multi-agent workflows
- ‚úÖ Production-grade performance (perfect O(n) scaling)
- ‚úÖ Sandboxed tool execution
- ‚úÖ Minimal memory footprint
- ‚úÖ Clean architecture (SOLID principles, 9.5/10 score)
- ‚úÖ Service-oriented design with dependency injection

### World-Class Multi-Agent

- ‚úÖ Real-time event streaming
- ‚úÖ Live progress tracking
- ‚úÖ DAG and Autonomous execution modes
- ‚úÖ Perfect linear scaling
- ‚úÖ Production-tested and benchmarked

### Extensible

Every component is pluggable:
- Custom LLM providers
- Custom tools
- Custom reasoning engines
- Custom workflow executors
- Custom extensions

All through clean interfaces, no fork needed.

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">
  
**Hector** - Declarative AI Agent Framework

*Define once, deploy anywhere. Configuration over code.*

Built with Go ‚Ä¢ MIT License ‚Ä¢ Alpha Stage

</div>

